{
	"name": "Temporary_Fix_ddce44ea-ac48-45fb-ae88-8d1af5003e25",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "backuppool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "86e8ea7a-41d4-4a85-ba59-1799b3dae1ae"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/e6a89ab3-ce27-4148-bad8-97a6fee2010c/resourceGroups/FergusAssam/providers/Microsoft.Synapse/workspaces/water-crisis/bigDataPools/backuppool",
				"name": "backuppool",
				"type": "Spark",
				"endpoint": "https://water-crisis.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/backuppool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.4",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# <div style=\"color:#fff;display:fill;border-radius:15px;background-color:#328ada;text-align:left;letter-spacing:0.1px;overflow:hidden;padding:5px;color:white;overflow:hidden;margin:0;font-size:80%\">Epic Internship</div>"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## <div style=\"color:#fff;display:fill;border-radius:10px;background-color:#328ada;text-align:left;letter-spacing:0.1px;overflow:hidden;padding:20px;color:white;overflow:hidden;margin:0;font-size:80%\"> üîßüìóET Code for the \"üèûÔ∏èCatalonia Water Resource Dailyüö∞\" Dataset.</div>\r\n",
					"\r\n",
					"This Notebook is, **scheduled** to run automatically on a **daily** basis in a pipeline, in order to extract information from the public API of the [ACA - Ag√®ncia Catalana de l'Aigua](https://aca.gencat.cat/) (Catalan Water Agency). This API contains the **real-time value reads from 4 types of sensors** (Reservoir Level Sensors, Gauging Sensors, Pluviometer Sensors, and Piezometer Sensors) placed around the **Catalan region** in Spain. It also contains Metadata information which is extracted and saved in another supplementary Notebook(copy_metadata_monthly), and later imported here.\r\n",
					"\r\n",
					"#### Internship Business Case:\r\n",
					"\r\n",
					"Catalonia is currently experiencing a dry period, characterized by reduced precipitation and lower-than-average water levels in reservoirs and rivers as well as in other regions. Struggling with water reserve levels already, the goal of this internship is to come up with actionable insights that will help the government make informed decisions and solutions that will benefit the community and the region at large.\r\n",
					"\r\n",
					"##### Note:\r\n",
					"Data is uploaded from the sensors at different time intervals but the data is collected and average taken for the closet time on all the 4 categories.\r\n",
					"\r\n",
					"## <div style=\"color:#fff;display:fill;border-radius:15px;background-color:#328ada;text-align:left;letter-spacing:0.1px;overflow:hidden;padding:5px;color:white;overflow:hidden;margin:0;font-size:80%\"></div>\r\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Parameters and Libraries"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Create a parameter that will be used to change read folder and mode dynamically in the pipeline"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"# Get the version of the folder from the Bronze layer and this will be used as a parameter in the pipeline (Toggled as a parameter)\r\n",
					"#oldFolder = \"Version_01\"\r\n",
					"FolderName = \"Version_01\"\r\n",
					"Mode = \"overwrite\" \r\n",
					"MetaFolder = \"Metadata\"\r\n",
					"category = ['reservoir','gauge','pluviometer', 'piezometer']\r\n",
					"meta_end = '_sensors_metadata'\r\n",
					"meta_path = f\"abfss://waterstorageaccount@projectwatercrisis.dfs.core.windows.net/BronzeLayer/{FolderName}/{MetaFolder}\"\r\n",
					"sensors_reads_path = f\"abfss://waterstorageaccount@projectwatercrisis.dfs.core.windows.net/BronzeLayer/{FolderName}\"\r\n",
					"sensors_end = '_sensors_reads'\r\n",
					"ml_path = f\"abfss://waterstorageaccount@projectwatercrisis.dfs.core.windows.net/ML_Folder\"\r\n",
					"#For the first time it overwrites and for subsequent notebooks, it apppends which is adjusted via the pipeline\r\n",
					"# Also live data can be extracted from the API."
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Check If Folder Exists"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Check if the file path exists using mssparkutils\r\n",
					"if not mssparkutils.fs.exists(f\"{sensors_reads_path}\"):\r\n",
					"    print(f\"Folder {FolderName} Not Found, Creating One Now \")\r\n",
					"    mssparkutils.fs.mkdirs(f\"{sensors_reads_path}\")\r\n",
					"else:\r\n",
					"    print(f\"Folder {FolderName} Found, Proceeding with code\")"
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Generate the dataset Folder incrementally"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# FolderName_SeperatorRemoved = FolderName.replace(\"_\",\"\")\r\n",
					"# VersionName = FolderName.split('_')[0]\r\n",
					"# VersionNum = FolderName.split('_')[1]"
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Import Libraries"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Weather API"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"\r\n",
					"pip install openmeteo-requests"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"pip install requests-cache retry-requests"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql import SparkSession\r\n",
					"from pyspark.sql import functions as F, Row\r\n",
					"from pyspark.sql.functions import lit,col,regexp_replace,to_date,count,sum,coalesce,to_timestamp, current_date, date_sub #add constant or literal value as a new column to the DataFrame. #regexp_replace to replace a character in a string\r\n",
					"from pyspark.sql.types import DateType\r\n",
					"from datetime import datetime, timedelta\r\n",
					"from tqdm.notebook import tqdm\r\n",
					"import re\r\n",
					"#from pathlib import Path\r\n",
					"import matplotlib.pyplot as plt\r\n",
					"from scipy.stats import zscore\r\n",
					"import pandas as pd\r\n",
					"import numpy as np\r\n",
					"import requests,json,os\r\n",
					"from notebookutils import mssparkutils\r\n",
					"import openmeteo_requests\r\n",
					"from retry_requests import retry\r\n",
					"import requests_cache\r\n",
					"spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\r\n",
					"#If you check the schema table, you will notice the column Timestamp is stored as (M.D.Y.hh.mm). \r\n",
					"#inorder to handle unforseen exception error. eg October, Oct ,oct, 10, we use the timeParserPolicy=LEGACY to fix this."
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Create Connections"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Spark Session & blob connection\r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Create Spark Session and connection to blob storage. It is created in the Notebook and not in the pipeline. \r\n",
					"#You can only set Spark configuration properties that start with the spark.sql prefix.\r\n",
					"# \"\"\"You can have many spark sessions but only 1 spark context (like spark driver). It is better in the notebook because\r\n",
					"# It already has access to the underlying Spark features for creating RDD and dataframe.\"\"\"\r\n",
					"\r\n",
					"blob_account = \"waterstorageaccount\"\r\n",
					"blob_container = \"BronzeLayer\"\r\n",
					"\r\n",
					"sc = SparkSession.builder.getOrCreate()\r\n",
					"#getConnectionString()\r\n",
					"token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\r\n",
					"dls_sas_token = token_library.getConnectionString(\"water-crisis-WorkspaceDefaultStorage\")\r\n",
					"spark.conf.set(\r\n",
					"    'fs.azure.sas.{}.{}.blob.core.windows.net' .format(blob_container, blob_account),\r\n",
					"    dls_sas_token)\r\n",
					"\r\n",
					"#https://projectwatercrisis.dfs.core.windows.net/waterstorageaccount/BronzeLayer/Version_322/gauge_sensors_reads.csv\r\n",
					""
				],
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Copy Previous Data To New Folder For Append\r\n",
					"\r\n",
					"will be done in pipeline\r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# # list sub folders\r\n",
					"# mssparkutils.fs.ls(f\"abfss://waterstorageaccount@projectwatercrisis.dfs.core.windows.net/BronzeLayer/\")\r\n",
					"\r\n",
					"# # Create New Folder\r\n",
					"# mssparkutils.fs.mkdirs(f\"abfss://waterstorageaccount@projectwatercrisis.dfs.core.windows.net/BronzeLayer/{FolderName}\")\r\n",
					"\r\n",
					"# # Copy Old data to New Folder\r\n",
					"# old_data_path = f\"abfss://waterstorageaccount@projectwatercrisis.dfs.core.windows.net/BronzeLayer/{oldFolder}/*\"\r\n",
					"# new_folder_path = f\"abfss://waterstorageaccount@projectwatercrisis.dfs.core.windows.net/BronzeLayer/{FolderName}\"\r\n",
					"# mssparkutils.fs.fastcp(f\"{old_data_path}\", f\"{new_folder_path}\", True) # Set the third parameter as True to copy all files and directories recursively"
				],
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Dynamic Delimiter"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Use Regex to read the header, determine the delimiter from a set of predefined common delimiters \r\n",
					"# and store it as a variable then use that variable as a delimiter\r\n",
					"def f_get_delimiter (source_path):\r\n",
					"    try:\r\n",
					"        print(\"Source Path:\", source_path)  # Debug: Print the source path\r\n",
					"        headerlist = spark.sparkContext.textFile(source_path).take(1)\r\n",
					"        header_str = ''.join(headerlist)\r\n",
					"\r\n",
					"        results= re.search(\"(,|;|\\\\|)\",header_str)\r\n",
					"        return results.group()\r\n",
					"    except Exception as err:\r\n",
					"        print(\"Error Occured \", str(err))"
				],
				"execution_count": 9
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## METADATA"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### We have data available from 4 different types of sensors:\r\n",
					"- Reservoir Sensors <br>\r\n",
					"- Gauging Sensors <br>\r\n",
					"- Pluviomter Sensors <br>\r\n",
					"- Piezometer Sensors"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Metadata Links"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# ACA API endpoints\r\n",
					"# https://aca.gencat.cat/ca/laigua/consulta-de-dades/dades-obertes/dades-obertes-temps-real/index.html#googtrans(ca|en)\r\n",
					"\r\n",
					"# Info Catalog URLs [Metadata]\r\n",
					"URL_RESERVOIRS_CATALOG = \"http://aca-web.gencat.cat/sdim2/apirest/catalog?componentType=embassament\"\r\n",
					"URL_GAUGE_CATALOG = \"http://aca-web.gencat.cat/sdim2/apirest/catalog?componentType=aforament\"\r\n",
					"URL_PLUVIOMETER_CATALOG = \"http://aca-web.gencat.cat/sdim2/apirest/catalog?componentType=pluviometre\"\r\n",
					"URL_PIEZOMETER_CATALOG = \"http://aca-web.gencat.cat/sdim2/apirest/catalog?componentType=piezometre\""
				],
				"execution_count": 10
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Load Meta API Files"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def get_metadata(metaurl):\r\n",
					"    # Make a GET request to the URL\r\n",
					"    meta_response = requests.get(metaurl)\r\n",
					"\r\n",
					"    # Check if the request was successful (status code 200)\r\n",
					"    if meta_response.status_code == 200:\r\n",
					"        # Parse the JSON response\r\n",
					"        json_metadata = meta_response.json()\r\n",
					"        return json_metadata\r\n",
					"    \r\n",
					"    else:\r\n",
					"        print(f\"Request failed with status code {meta_response.status_code}\")  \r\n",
					"        return None\r\n",
					"    \r\n",
					"\r\n",
					"# Check if the file path exists using mssparkutils\r\n",
					"if not mssparkutils.fs.exists(f\"{meta_path}\"):\r\n",
					"    print(\"No Meta Folder Found, Creating now \")\r\n",
					"    mssparkutils.fs.mkdirs(f\"{meta_path}\")"
				],
				"execution_count": 11
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### 2.1. Reservoir Sensors Metadata¬∂\r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"json_reservoir_sensors_metadata = get_metadata(URL_RESERVOIRS_CATALOG)\r\n",
					"print(len(json_reservoir_sensors_metadata[\"providers\"]))\r\n",
					"print(json_reservoir_sensors_metadata[\"providers\"][0].keys())\r\n",
					"\r\n",
					"\r\n",
					"# Exploring 3rd-4th levels (sensors)\r\n",
					"print(\"Provider:\", json_reservoir_sensors_metadata[\"providers\"][0][\"provider\"])\r\n",
					"print(\"Permission:\", json_reservoir_sensors_metadata[\"providers\"][0][\"permission\"])\r\n",
					"print(\"Num  sensors:\", len(json_reservoir_sensors_metadata[\"providers\"][0][\"sensors\"]))\r\n",
					"print(\"\\n\\tSensor keys:\", json_reservoir_sensors_metadata[\"providers\"][0][\"sensors\"][0].keys())"
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"print(json_reservoir_sensors_metadata[\"providers\"][0][\"sensors\"][3][\"componentAdditionalInfo\"])\r\n",
					"#3 = row"
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Exploring a sample sensor\"s info\r\n",
					"\r\n",
					"for k in json_reservoir_sensors_metadata[\"providers\"][0][\"sensors\"][0].keys():\r\n",
					"    print(f\"\\n{k}  -->  {json_reservoir_sensors_metadata['providers'][0]['sensors'][0][k]}\")"
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Building the dataframe to store all sensors data\r\n",
					"\r\n",
					"translate = {\r\n",
					"    \"Nivell absolut\": \"Absolute level\",\r\n",
					"    \"Volum embassat\": \"Reservoir volume\",\r\n",
					"    \"Percentatge volum embassat\": \"Percentage reservoir volume\",\r\n",
					"    \"En servei\": \"In service\",\r\n",
					"    \"En manteniment\": \"In maintenance\",\r\n",
					"    \"Fora de servei\": \"Out of service\",\r\n",
					"    \"Funcionalitat limitada\": \"Limited functionality\",\r\n",
					"}\r\n",
					"\r\n",
					"sensors_metadata = []\r\n",
					"\r\n",
					"for i, s in enumerate(json_reservoir_sensors_metadata[\"providers\"][0][\"sensors\"]):\r\n",
					"    sensor_metadata = {\r\n",
					"        \"sensor_id\": s[\"sensor\"],\r\n",
					"        \"description\": translate[s[\"description\"]] if s[\"description\"] in translate else s[\"description\"],\r\n",
					"        \"dataType\": s[\"dataType\"],\r\n",
					"        \"location\": s[\"location\"],\r\n",
					"        \"type\": s[\"type\"],\r\n",
					"        \"unit\": \"masl\" if s[\"unit\"] == \"msnm\" else s[\"unit\"],\r\n",
					"        \"timeZone\": s[\"timeZone\"],\r\n",
					"        \"publicAccess\": s[\"publicAccess\"],\r\n",
					"        \"component\": s[\"component\"],\r\n",
					"        \"componentType\": \"reservoir\" if s[\"componentType\"] == \"embassament\" else s[\"componentType\"],\r\n",
					"        \"componentDesc\": s[\"componentDesc\"],\r\n",
					"        \"componentPublicAccess\": s[\"componentPublicAccess\"],\r\n",
					"        \"info_sample_rate_(min)\": s[\"additionalInfo\"][\"Temps mostreig (min)\"] if \"additionalInfo\" in s else np.nan,\r\n",
					"        \"info_min_val\": int(s[\"additionalInfo\"][\"Rang m√≠nim\"].replace(\",\", \".\")) if \"additionalInfo\" in s else np.nan,\r\n",
					"        \"info_max_val\": int(s[\"additionalInfo\"][\"Rang m√†xim\"].replace(\",\", \".\")) if \"additionalInfo\" in s else np.nan,\r\n",
					"        \"info_region\": s[\"componentAdditionalInfo\"][\"Comarca\"] if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_province\": s[\"componentAdditionalInfo\"][\"Prov√≠ncia\"] if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_river\": s[\"componentAdditionalInfo\"][\"Riu\"] if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_river_district\": s[\"componentAdditionalInfo\"][\"Districte fluvial\"] if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_basin\": s[\"componentAdditionalInfo\"][\"Conca\"] if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_sub_basin\": s[\"componentAdditionalInfo\"][\"Subconca\"] if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_municipality\": s[\"componentAdditionalInfo\"][\"Terme municipal\"] if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_reservoir_max_capacity_(hm3)\": float(s[\"componentAdditionalInfo\"][\"Capacitat m√†xima embassament\"].split(\" \")[0].replace(\",\", \".\")) if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_x_coord_utm_etrs89\": int(s[\"componentAdditionalInfo\"][\"Coordenada X (UTM ETRS89)\"]) if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_y_coord_utm_etrs89\": int(s[\"componentAdditionalInfo\"][\"Coordenada Y (UTM ETRS89)\"]) if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_drained_basin_surface(km2)\": float(s[\"componentAdditionalInfo\"][\"Superf√≠cie conca drenada\"].split(\" \")[0].replace(\",\", \".\")) if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_responsible_entity\": s[\"componentAdditionalInfo\"][\"Titular\"] if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_state\": (translate[s[\"componentAdditionalInfo\"][\"Estat\"]] if s[\"componentAdditionalInfo\"][\"Estat\"] in translate else s[\"componentAdditionalInfo\"][\"Estat\"]) if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"    }\r\n",
					"    sensors_metadata.append(sensor_metadata)\r\n",
					""
				],
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pandas as pd"
				],
				"execution_count": 16
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Reservoir DF"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"\r\n",
					"reservoir_sensors_metadata = pd.DataFrame(sensors_metadata)\r\n",
					"reservoir_sensors_metadata.to_csv(f\"{meta_path}/reservoir{meta_end}.csv\", index=False)\r\n",
					"\r\n",
					"print(reservoir_sensors_metadata.shape)\r\n",
					"display(reservoir_sensors_metadata.info())\r\n",
					"reservoir_sensors_metadata.head()"
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#print sample\r\n",
					"reservoir_sensors_metadata.iloc[1]\r\n",
					""
				],
				"execution_count": 18
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### 2.2. Gauge Sensors Metadata¬∂\r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Getting Gauge Sensors Metadata from the API\r\n",
					"json_gauge_sensors_metadata = get_metadata(URL_GAUGE_CATALOG)\r\n",
					"\r\n",
					"# Exploring 1st-2nd levels of the json hierarchy\r\n",
					"print(len(json_gauge_sensors_metadata[\"providers\"]))\r\n",
					"print(json_gauge_sensors_metadata[\"providers\"][0].keys())\r\n",
					"\r\n",
					"# Exploring 3rd-4th levels (sensors)\r\n",
					"print(\"Provider:\", json_gauge_sensors_metadata[\"providers\"][0][\"provider\"])\r\n",
					"print(\"Permission:\", json_gauge_sensors_metadata[\"providers\"][0][\"permission\"])\r\n",
					"print(\"Num  sensors:\", len(json_gauge_sensors_metadata[\"providers\"][0][\"sensors\"]))\r\n",
					"print(\"\\n\\tSensor keys:\", list(json_gauge_sensors_metadata[\"providers\"][0][\"sensors\"][0].keys()))"
				],
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Exploring a sample sensor\"s info\r\n",
					"\r\n",
					"for k in json_gauge_sensors_metadata[\"providers\"][0][\"sensors\"][0].keys():\r\n",
					"    print(f\"\\n{k}  -->  {json_gauge_sensors_metadata['providers'][0]['sensors'][0][k]}\")"
				],
				"execution_count": 20
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Exploring a sample sensor\"s info\r\n",
					"\r\n",
					"print(json_gauge_sensors_metadata[\"providers\"][0][\"sensors\"][15][\"componentAdditionalInfo\"])"
				],
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Building the dataframe to store all sensors data\r\n",
					"\r\n",
					"additional_fields_map = {\r\n",
					"    \"Codi ordre\": \"info_order_id\",\r\n",
					"    \"Comarca\": \"info_region\",\r\n",
					"    \"Conca\": \"info_basin\",\r\n",
					"    \"Coordenada X (UTM ETRS89)\": \"info_x_coord_utm_etrs89\",\r\n",
					"    \"Coordenada Y (UTM ETRS89)\": \"info_y_coord_utm_etrs89\",\r\n",
					"    \"Districte fluvial\": \"info_river_district\",\r\n",
					"    \"Estat\": \"info_state\",\r\n",
					"    \"Prov√≠ncia\": \"info_province\",\r\n",
					"    \"Riu\": \"info_river\",\r\n",
					"    \"Subconca\": \"info_sub_basin\",\r\n",
					"    \"Superf√≠cie conca drenada\": \"info_drained_basin_surface(km2)\",\r\n",
					"    \"Terme municipal\": \"info_municipality\",\r\n",
					"    \"Tipologia\": \"info_tipology\",\r\n",
					"    \"Titular\": \"info_responsible_entity\",\r\n",
					"}\r\n",
					"\r\n",
					"translate = {\r\n",
					"    \"Cabal riu\": \"River discharge\",\r\n",
					"    \"Nivell riu\": \"River level\",\r\n",
					"    \"Cabal canal\": \"Canal discharge\",\r\n",
					"    \"Cabal total\": \"Total discharge\",\r\n",
					"    \"En servei\": \"In service\",\r\n",
					"    \"En manteniment\": \"In maintenance\",\r\n",
					"    \"Fora de servei\": \"Out of service\",\r\n",
					"    \"Funcionalitat limitada\": \"Limited functionality\",\r\n",
					"}\r\n",
					"\r\n",
					"sensors_metadata = []\r\n",
					"\r\n",
					"for i, s in enumerate(json_gauge_sensors_metadata[\"providers\"][0][\"sensors\"]):\r\n",
					"    sensor_metadata = {\r\n",
					"        \"sensor_id\": s[\"sensor\"],\r\n",
					"        \"description\": translate[s[\"description\"]] if s[\"description\"] in translate else s[\"description\"],\r\n",
					"        \"dataType\": s[\"dataType\"],\r\n",
					"        \"location\": s[\"location\"],\r\n",
					"        \"type\": s[\"type\"],\r\n",
					"        \"unit\": s[\"unit\"],\r\n",
					"        \"timeZone\": s[\"timeZone\"],\r\n",
					"        \"publicAccess\": s[\"publicAccess\"],\r\n",
					"        \"component\": s[\"component\"],\r\n",
					"        \"componentType\": \"Capacity\" if s[\"componentType\"] == \"aforament\" else s[\"componentType\"],\r\n",
					"        \"componentDesc\": s[\"componentDesc\"],\r\n",
					"        \"componentPublicAccess\": s[\"componentPublicAccess\"],\r\n",
					"        \"info_sample_rate_(min)\": s[\"additionalInfo\"][\"Temps mostreig (min)\"] if \"additionalInfo\" in s else np.nan,\r\n",
					"        \"info_min_val\": int(s[\"additionalInfo\"][\"Rang m√≠nim\"].replace(\",\", \".\")) if \"additionalInfo\" in s else np.nan,\r\n",
					"        \"info_max_val\": int(s[\"additionalInfo\"][\"Rang m√†xim\"].replace(\",\", \".\")) if \"additionalInfo\" in s else np.nan,\r\n",
					"    }\r\n",
					"    \r\n",
					"    if \"componentAdditionalInfo\" in s:\r\n",
					"        for field_k, field_v in additional_fields_map.items():\r\n",
					"            if field_k in s[\"componentAdditionalInfo\"]:\r\n",
					"                \r\n",
					"                if field_v in [\"info_x_coord_utm_etrs89\", \"info_y_coord_utm_etrs89\"]:\r\n",
					"                    sensor_metadata[field_v] = int(s[\"componentAdditionalInfo\"][field_k])\r\n",
					"                    \r\n",
					"                elif field_v == \"info_drained_basin_surface(km2)\":\r\n",
					"                    sensor_metadata[field_v] = float(s[\"componentAdditionalInfo\"][field_k].split(\" \")[0].replace(\",\", \".\")) if s[\"componentAdditionalInfo\"][field_k] else np.nan\r\n",
					"                    \r\n",
					"                elif field_v == \"info_state\":\r\n",
					"                    sensor_metadata[field_v] = (translate[s[\"componentAdditionalInfo\"][\"Estat\"]] if s[\"componentAdditionalInfo\"][\"Estat\"] in translate else s[\"componentAdditionalInfo\"][\"Estat\"]) if \"componentAdditionalInfo\" in s else np.nan\r\n",
					"                \r\n",
					"                else:\r\n",
					"                    sensor_metadata[field_v] = s[\"componentAdditionalInfo\"][field_k]\r\n",
					"                    \r\n",
					"            else: \r\n",
					"                sensor_metadata[field_v] = np.nan\r\n",
					"                \r\n",
					"    sensors_metadata.append(sensor_metadata)\r\n",
					""
				],
				"execution_count": 22
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Gauge DF"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"gauge_sensors_metadata = pd.DataFrame(sensors_metadata)\r\n",
					"gauge_sensors_metadata.to_csv(f\"{meta_path}/gauge{meta_end}.csv\", index=False)\r\n",
					"\r\n",
					"gauge_sensors_metadata.head()"
				],
				"execution_count": 23
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Comparing different sample types\r\n",
					"# There are two types of river sensors: gauge (l/s) and level (cm)\r\n",
					"\r\n",
					"print(gauge_sensors_metadata.iloc[0]), \r\n",
					"print(\"-\" * 60), \r\n",
					"print(gauge_sensors_metadata.iloc[1])\r\n",
					"print(\"-\" * 60), \r\n",
					"print(gauge_sensors_metadata.iloc[15])"
				],
				"execution_count": 24
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"gauge_sensors_metadata[\"componentType\"].value_counts()"
				],
				"execution_count": 25
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### 2.3. Pluviometer Sensors Metadata¬∂\r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Getting Gauge Sensors Metadata from the API\r\n",
					"json_pluviometer_sensors_metadata = get_metadata(URL_PLUVIOMETER_CATALOG)\r\n",
					"\r\n",
					"# Exploring 1st-2nd levels of the json hierarchy\r\n",
					"print(len(json_pluviometer_sensors_metadata[\"providers\"]))\r\n",
					"print(json_pluviometer_sensors_metadata[\"providers\"][0].keys())\r\n",
					"\r\n",
					"# Exploring 3rd-4th levels (sensors)\r\n",
					"print(\"Provider:\", json_pluviometer_sensors_metadata[\"providers\"][0][\"provider\"])\r\n",
					"print(\"Permission:\", json_pluviometer_sensors_metadata[\"providers\"][0][\"permission\"])\r\n",
					"print(\"Num  sensors:\", len(json_pluviometer_sensors_metadata[\"providers\"][0][\"sensors\"]))\r\n",
					"print(\"\\n\\tSensor keys:\", list(json_pluviometer_sensors_metadata[\"providers\"][0][\"sensors\"][0].keys()))"
				],
				"execution_count": 26
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Exploring a sample sensor\"s info\r\n",
					"\r\n",
					"for k in json_pluviometer_sensors_metadata[\"providers\"][0][\"sensors\"][0].keys():\r\n",
					"    print(f\"\\n{k}  -->  {json_pluviometer_sensors_metadata['providers'][0]['sensors'][0][k]}\")"
				],
				"execution_count": 27
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Exploring a sample sensor\"s info\r\n",
					"\r\n",
					"print(*[k + \"\\n\" for k in json_pluviometer_sensors_metadata[\"providers\"][0][\"sensors\"][1][\"componentAdditionalInfo\"]])"
				],
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"info_fields = []\r\n",
					"\r\n",
					"for s in json_pluviometer_sensors_metadata[\"providers\"][0][\"sensors\"]:\r\n",
					"#     print(s[\"description\"])\r\n",
					"#     continue\r\n",
					"    info_fields.extend(s[\"componentAdditionalInfo\"])\r\n",
					"    print(len(s[\"componentAdditionalInfo\"]), end=\", \")\r\n",
					"\r\n",
					"print(\"\\n\\nunique:\", len(set(info_fields)))\r\n",
					"set(info_fields)"
				],
				"execution_count": 29
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Building the dataframe to store all sensors data\r\n",
					"\r\n",
					"translate = {\r\n",
					"    \"Nivell piezom√®tric absolut\": \"Absolute piezometric level\",\r\n",
					"    \"Temperatura de l'aigua\": \"Water temperature\",\r\n",
					"    \"Conductivita\": \"Conductivity\",\r\n",
					"    \"En servei\": \"In service\",\r\n",
					"    \"En manteniment\": \"In maintenance\",\r\n",
					"    \"Fora de servei\": \"Out of service\",\r\n",
					"    \"Funcionalitat limitada\": \"Limited functionality\",\r\n",
					"}\r\n",
					"\r\n",
					"sensors_metadata = []\r\n",
					"\r\n",
					"for i, s in enumerate(json_pluviometer_sensors_metadata[\"providers\"][0][\"sensors\"]):\r\n",
					"    sensor_metadata = {\r\n",
					"        \"sensor_id\": s[\"sensor\"],\r\n",
					"        \"description\": \"Rainfall intensity\" if s[\"description\"] == \"Intensitat de precipitaci√≥\" else s[\"description\"],\r\n",
					"        \"dataType\": s[\"dataType\"],\r\n",
					"        \"location\": s[\"location\"],\r\n",
					"        \"type\": s[\"type\"],\r\n",
					"        \"unit\": s[\"unit\"],\r\n",
					"        \"timeZone\": s[\"timeZone\"],\r\n",
					"        \"publicAccess\": s[\"publicAccess\"],\r\n",
					"        \"component\": s[\"component\"],\r\n",
					"        \"componentType\": \"reservoir\" if s[\"componentType\"] == \"embassament\" else s[\"componentType\"],\r\n",
					"        \"componentDesc\": s[\"componentDesc\"],\r\n",
					"        \"componentPublicAccess\": s[\"componentPublicAccess\"],\r\n",
					"        \"info_sample_rate_(min)\": s[\"additionalInfo\"][\"Temps mostreig (min)\"] if \"additionalInfo\" in s else np.nan,\r\n",
					"        \"info_min_val\": int(s[\"additionalInfo\"][\"Rang m√≠nim\"].replace(\",\", \".\")) if \"additionalInfo\" in s else np.nan,\r\n",
					"        \"info_max_val\": int(s[\"additionalInfo\"][\"Rang m√†xim\"].replace(\",\", \".\")) if \"additionalInfo\" in s else np.nan,\r\n",
					"        \"info_region\": s[\"componentAdditionalInfo\"][\"Comarca\"] if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_province\": s[\"componentAdditionalInfo\"][\"Prov√≠ncia\"] if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_river\": s[\"componentAdditionalInfo\"][\"Riu\"] if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_river_district\": s[\"componentAdditionalInfo\"][\"Districte fluvial\"] if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_basin\": s[\"componentAdditionalInfo\"][\"Conca\"] if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_sub_basin\": s[\"componentAdditionalInfo\"][\"Subconca\"] if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_municipality\": s[\"componentAdditionalInfo\"][\"Terme municipal\"] if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_x_coord_utm_etrs89\": int(s[\"componentAdditionalInfo\"][\"Coordenada X (UTM ETRS89)\"]) if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_y_coord_utm_etrs89\": int(s[\"componentAdditionalInfo\"][\"Coordenada Y (UTM ETRS89)\"]) if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_state\": (translate[s[\"componentAdditionalInfo\"][\"Estat\"]] if s[\"componentAdditionalInfo\"][\"Estat\"] in translate else s[\"componentAdditionalInfo\"][\"Estat\"]) if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"    }\r\n",
					"    \r\n",
					"    sensors_metadata.append(sensor_metadata)"
				],
				"execution_count": 30
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Pluviometer DF"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"pluviometer_sensors_metadata = pd.DataFrame(sensors_metadata)\r\n",
					"pluviometer_sensors_metadata.to_csv(f\"{meta_path}/pluviometer{meta_end}.csv\", index=False)\r\n",
					"\r\n",
					"pluviometer_sensors_metadata.head()"
				],
				"execution_count": 31
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"pluviometer_sensors_metadata.iloc[0]\r\n",
					""
				],
				"execution_count": 32
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### 2.4. Piezometer Sensors Metadata¬∂\r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Getting Piezometer Sensors Metadata from the API\r\n",
					"json_piezometer_sensors_metadata = get_metadata(URL_PIEZOMETER_CATALOG)\r\n",
					"\r\n",
					"# Exploring 1st-2nd levels of the json hierarchy\r\n",
					"print(len(json_piezometer_sensors_metadata[\"providers\"]))\r\n",
					"print(json_piezometer_sensors_metadata[\"providers\"][0].keys())\r\n",
					"\r\n",
					"\r\n",
					"# Exploring 3rd-4th levels (sensors)\r\n",
					"print(\"Provider:\", json_piezometer_sensors_metadata[\"providers\"][0][\"provider\"])\r\n",
					"print(\"Permission:\", json_piezometer_sensors_metadata[\"providers\"][0][\"permission\"])\r\n",
					"print(\"Num  sensors:\", len(json_piezometer_sensors_metadata[\"providers\"][0][\"sensors\"]))\r\n",
					"print(\"\\n\\tSensor keys:\", list(json_piezometer_sensors_metadata[\"providers\"][0][\"sensors\"][0].keys()))"
				],
				"execution_count": 33
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Exploring a sample sensor\"s info\r\n",
					"\r\n",
					"for k in json_piezometer_sensors_metadata[\"providers\"][0][\"sensors\"][0].keys():\r\n",
					"    print(f\"\\n{k}  -->  {json_piezometer_sensors_metadata['providers'][0]['sensors'][0][k]}\")"
				],
				"execution_count": 34
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Exploring a sample sensor\"s info\r\n",
					"\r\n",
					"print(*[k + \"\\n\" for k in json_piezometer_sensors_metadata[\"providers\"][0][\"sensors\"][1][\"componentAdditionalInfo\"]])"
				],
				"execution_count": 35
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"info_fields = []\r\n",
					"\r\n",
					"for i, s in enumerate(json_piezometer_sensors_metadata[\"providers\"][0][\"sensors\"]):\r\n",
					"#     print(s[\"description\"])\r\n",
					"#     continue\r\n",
					"    info_fields.extend(s[\"componentAdditionalInfo\"]) if \"componentAdditionalInfo\" in s else print(\"no componentAdditionalInfo in\", i)\r\n",
					"    print(len(s[\"componentAdditionalInfo\"]) if \"componentAdditionalInfo\" in s else \"\", end=\", \")\r\n",
					"\r\n",
					"print(\"\\n\\nunique:\", len(set(info_fields)))\r\n",
					"set(info_fields)"
				],
				"execution_count": 36
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Building the dataframe to store all sensors data\r\n",
					"\r\n",
					"translate = {\r\n",
					"    \"Nivell piezom√®tric absolut\": \"Absolute piezometric level\",\r\n",
					"    \"Temperatura de l'aigua\": \"Water temperature\",\r\n",
					"    \"Conductivitat\": \"Conductivity\",\r\n",
					"    \"En servei\": \"In service\",\r\n",
					"    \"En manteniment\": \"In maintenance\",\r\n",
					"    \"Fora de servei\": \"Out of service\",\r\n",
					"    \"Funcionalitat limitada\": \"Limited functionality\",\r\n",
					"    \"msnm\": \"masl\",\r\n",
					"    r\"\\xbaC\": \"¬∞C\",\r\n",
					"    r\"\\xb5S/cm\": \"¬µS/cm\",  # microsiemens/cm\r\n",
					"}\r\n",
					"\r\n",
					"sensors_metadata = []\r\n",
					"\r\n",
					"for i, s in enumerate(json_piezometer_sensors_metadata[\"providers\"][0][\"sensors\"]):\r\n",
					"    sensor_metadata = {\r\n",
					"        \"sensor_id\": s[\"sensor\"],\r\n",
					"        \"description\": translate[s[\"description\"]] if s[\"description\"] in translate else s[\"description\"],\r\n",
					"        \"dataType\": s[\"dataType\"],\r\n",
					"        \"location\": s[\"location\"],\r\n",
					"        \"type\": s[\"type\"],\r\n",
					"        \"unit\": translate[s[\"unit\"]] if s[\"unit\"] in translate else s[\"unit\"],\r\n",
					"        \"timeZone\": s[\"timeZone\"],\r\n",
					"        \"publicAccess\": s[\"publicAccess\"],\r\n",
					"        \"component\": s[\"component\"],\r\n",
					"        \"componentType\": \"reservoir\" if s[\"componentType\"] == \"embassament\" else s[\"componentType\"],\r\n",
					"        \"componentDesc\": s[\"componentDesc\"],\r\n",
					"        \"componentPublicAccess\": s[\"componentPublicAccess\"],\r\n",
					"        \"info_sample_rate_(min)\": s[\"additionalInfo\"][\"Temps mostreig (min)\"] if \"additionalInfo\" in s else np.nan,\r\n",
					"        \"info_min_val\": int(s[\"additionalInfo\"][\"Rang m√≠nim\"].replace(\",\", \".\")) if \"additionalInfo\" in s else np.nan,\r\n",
					"        \"info_max_val\": int(s[\"additionalInfo\"][\"Rang m√†xim\"].replace(\",\", \".\")) if \"additionalInfo\" in s else np.nan,\r\n",
					"        \"info_region\": s[\"componentAdditionalInfo\"][\"Comarca\"] if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_province\": s[\"componentAdditionalInfo\"][\"Prov√≠ncia\"] if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_aquifer\": s[\"componentAdditionalInfo\"][\"Aq√º√≠fer\"] if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_water_mass\": s[\"componentAdditionalInfo\"][\"Massa d'aigua\"] if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_river_district\": s[\"componentAdditionalInfo\"][\"Districte fluvial\"] if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_basin\": s[\"componentAdditionalInfo\"][\"Conca\"] if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_sub_basin\": s[\"componentAdditionalInfo\"][\"Subconca\"] if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_municipality\": s[\"componentAdditionalInfo\"][\"Terme municipal\"] if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_x_coord_utm_etrs89\": int(s[\"componentAdditionalInfo\"][\"Coordenada X (UTM ETRS89)\"]) if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_y_coord_utm_etrs89\": int(s[\"componentAdditionalInfo\"][\"Coordenada Y (UTM ETRS89)\"]) if \"componentAdditionalInfo\" in s else np.nan,\r\n",
					"        \"info_state\": (translate[s[\"componentAdditionalInfo\"][\"Estat\"]] if s[\"componentAdditionalInfo\"][\"Estat\"] in translate else s[\"componentAdditionalInfo\"][\"Estat\"]) if \"componentAdditionalInfo\" in s else np.nan,    \r\n",
					"    }\r\n",
					"    \r\n",
					"    sensors_metadata.append(sensor_metadata)\r\n",
					""
				],
				"execution_count": 37
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Piezometer DF"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"\r\n",
					"piezometer_sensors_metadata = pd.DataFrame(sensors_metadata)\r\n",
					"piezometer_sensors_metadata.to_csv(f\"{meta_path}/piezometer{meta_end}.csv\", index=False)\r\n",
					"piezometer_sensors_metadata.head()\r\n",
					""
				],
				"execution_count": 38
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"piezometer_sensors_metadata.iloc[4]\r\n",
					""
				],
				"execution_count": 39
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### 2.5. Sensor States¬∂\r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"list_sensors_metadata = [\r\n",
					"    reservoir_sensors_metadata,\r\n",
					"    gauge_sensors_metadata,\r\n",
					"    pluviometer_sensors_metadata,\r\n",
					"    piezometer_sensors_metadata,\r\n",
					"]\r\n",
					"\r\n",
					"for df_sensor in list_sensors_metadata:\r\n",
					"    print(\"-\"*40)\r\n",
					"    print(df_sensor.componentType[0])\r\n",
					"    print(\"-\"*40)\r\n",
					"    print(df_sensor.info_state.value_counts(), \"\\n\")"
				],
				"execution_count": 40
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Assuming dfs_ dictionary is already populated as before\r\n",
					"types = ['Gauge','Reservoir','Pluviometer', 'Piezometer']\r\n",
					"for subtype in types:\r\n",
					"    # Dynamically create variable names and assign the corresponding DataFrame from dfs_\r\n",
					"    print (f'{subtype} metadata database created.')\r\n",
					"\r\n",
					"print(\"All databases successfully created\")"
				],
				"execution_count": 41
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Extract Sensor ids\r\n",
					"Sensor ids will be extracted from the metadata and used to create the sensor readings dataframe"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Reservoir"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"reservoir_sensors_ids = reservoir_sensors_metadata[\"sensor_id\"]\r\n",
					"\r\n",
					"print(\"\\nDescriptions:\")\r\n",
					"display(reservoir_sensors_metadata[\"description\"].value_counts())\r\n",
					"print(\"\\nUnits:\")\r\n",
					"display(reservoir_sensors_metadata[\"unit\"].value_counts())\r\n",
					"print(\"\\nReservoir DF:\")\r\n",
					"# Using the native show() method in PySpark to display the DataFrame\r\n",
					"# print(reservoir_sensors_metadata.head(5))\r\n",
					""
				],
				"execution_count": 42
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# # Get sensor IDs from the DataFrame\r\n",
					"# reservoir_sensors_ids = reservoir_sensors_metadata.select(\"sensor_id\").toPandas()['sensor_id'].tolist()\r\n",
					"# #print(\"\\nSensor IDs:\")\r\n",
					"# #reservoir_sensors_ids.show()\r\n",
					"\r\n",
					"# # Group by description and count occurrences\r\n",
					"# description_counts = reservoir_sensors_metadata.groupBy(\"description\").agg(count(\"*\").alias(\"count\"))\r\n",
					"# print(\"\\nDescriptions:\")\r\n",
					"# description_counts.show()\r\n",
					"\r\n",
					"# # # Group by unit and count occurrences\r\n",
					"# # unit_counts = reservoir_sensors_metadata.groupBy(\"unit\").agg(count(\"*\").alias(\"count\"))\r\n",
					"# # print(\"\\nUnits:\")\r\n",
					"# # unit_counts.show()"
				],
				"execution_count": 43
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Piezometer"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"piezometer_sensors_ids = piezometer_sensors_metadata[\"sensor_id\"]\r\n",
					"\r\n",
					"print(\"\\nDescriptions:\")\r\n",
					"display(piezometer_sensors_metadata[\"description\"].value_counts())\r\n",
					"print(\"\\nUnits:\")\r\n",
					"display(piezometer_sensors_metadata[\"unit\"].value_counts())"
				],
				"execution_count": 44
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# # Get sensor IDs from the DataFrame\r\n",
					"# piezometer_sensors_ids = piezometer_sensors_metadata.select(\"sensor_id\").toPandas()['sensor_id'].tolist()\r\n",
					"# #print(\"\\nSensor IDs:\")\r\n",
					"# #piezometer_sensors_ids.show()\r\n",
					"\r\n",
					"# # Group by description and count occurrences\r\n",
					"# description_counts = piezometer_sensors_metadata.groupBy(\"description\").agg(count(\"*\").alias(\"count\"))\r\n",
					"# print(\"\\nDescriptions:\")\r\n",
					"# description_counts.show()\r\n",
					"\r\n",
					"# # # Group by unit and count occurrences\r\n",
					"# # unit_counts = piezometer_sensors_metadata.groupBy(\"unit\").agg(count(\"*\").alias(\"count\"))\r\n",
					"# # print(\"\\nUnits:\")\r\n",
					"# # unit_counts.show()"
				],
				"execution_count": 45
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Pluviometer"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# # Get sensor IDs from the DataFrame\r\n",
					"# pluviometer_sensors_ids = pluviometer_sensors_metadata.select(\"sensor_id\").toPandas()['sensor_id'].tolist()\r\n",
					"# #print(\"\\nSensor IDs:\")\r\n",
					"# #pluviometer_sensors_ids.show()\r\n",
					"\r\n",
					"# # Group by description and count occurrences\r\n",
					"# description_counts = pluviometer_sensors_metadata.groupBy(\"description\").agg(count(\"*\").alias(\"count\"))\r\n",
					"# print(\"\\nDescriptions:\")\r\n",
					"# description_counts.show()\r\n",
					"\r\n",
					"# # # Group by unit and count occurrences\r\n",
					"# # unit_counts = pluviometer_sensors_metadata.groupBy(\"unit\").agg(count(\"*\").alias(\"count\"))\r\n",
					"# # print(\"\\nUnits:\")\r\n",
					"# # unit_counts.show()"
				],
				"execution_count": 46
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"pluviometer_sensors_ids = pluviometer_sensors_metadata[\"sensor_id\"]\r\n",
					"\r\n",
					"print(\"\\nDescriptions:\")\r\n",
					"display(pluviometer_sensors_metadata[\"description\"].value_counts())\r\n",
					"print(\"\\nUnits:\")\r\n",
					"display(pluviometer_sensors_metadata[\"unit\"].value_counts())"
				],
				"execution_count": 47
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### Gauge"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"gauge_sensors_ids = gauge_sensors_metadata[\"sensor_id\"]\r\n",
					"\r\n",
					"print(\"\\nDescriptions:\")\r\n",
					"display(gauge_sensors_metadata[\"description\"].value_counts())\r\n",
					"print(\"\\nUnits:\")\r\n",
					"display(gauge_sensors_metadata[\"unit\"].value_counts())"
				],
				"execution_count": 48
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# # Get sensor IDs from the DataFrame\r\n",
					"# gauge_sensors_ids = gauge_sensors_metadata.select(\"sensor_id\").toPandas()['sensor_id'].tolist()\r\n",
					"# #print(\"\\nSensor IDs:\")\r\n",
					"# #gauge_sensors_ids.show()\r\n",
					"\r\n",
					"# # Group by description and count occurrences\r\n",
					"# description_counts = gauge_sensors_metadata.groupBy(\"description\").agg(count(\"*\").alias(\"count\"))\r\n",
					"# print(\"\\nDescriptions:\")\r\n",
					"# description_counts.show()\r\n",
					"\r\n",
					"# # Group by unit and count occurrences\r\n",
					"# unit_counts = gauge_sensors_metadata.groupBy(\"unit\").agg(count(\"*\").alias(\"count\"))\r\n",
					"# print(\"\\nUnits:\")\r\n",
					"# unit_counts.show()"
				],
				"execution_count": 49
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# print(reservoir_sensors_ids)"
				],
				"execution_count": 50
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# The sensors ids will be the names of the columns of the DataFrames where the read values will be stored\r\n",
					"groups_cols_ids = [reservoir_sensors_ids, gauge_sensors_ids, pluviometer_sensors_ids, piezometer_sensors_ids]\r\n",
					"# print(groups_cols_ids)"
				],
				"execution_count": 51
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## SENSOR READS"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Data Extraction And Transformation"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### API Links and Extraction"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"groups_cols_ids = [reservoir_sensors_ids, gauge_sensors_ids, pluviometer_sensors_ids, piezometer_sensors_ids]\r\n",
					"\r\n",
					"\r\n",
					"# Sensors Data Endpoints\r\n",
					"URL_RESERVOIR_DATA = 'http://aca-web.gencat.cat/sdim2/apirest/data/EMBASSAMENT-EST'\r\n",
					"URL_GAUGE_DATA = 'http://aca-web.gencat.cat/sdim2/apirest/data/AFORAMENT-EST'\r\n",
					"URL_PLUVIOMETER_DATA = 'http://aca-web.gencat.cat/sdim2/apirest/data/PLUVIOMETREACA-EST'\r\n",
					"URL_PIEZOMETER_DATA = 'http://aca-web.gencat.cat/sdim2/apirest/data/PIEZOMETRE-EST'\r\n",
					"\r\n",
					"urls_data = [URL_RESERVOIR_DATA, URL_GAUGE_DATA, URL_PLUVIOMETER_DATA, URL_PIEZOMETER_DATA]\r\n",
					"\r\n",
					"# Function to extract data from API endpoints\r\n",
					"def extract_data_from_api(url):\r\n",
					"    response = requests.get(url)\r\n",
					"    \r\n",
					"    # Check if the request was successful (status code 200)\r\n",
					"    if response.status_code == 200:\r\n",
					"        json_data = response.json()\r\n",
					"        return json_data\r\n",
					"    else:\r\n",
					"        print(f\"Failed to fetch data from {url}: Status Code {response.status_code}\")\r\n",
					"        return None"
				],
				"execution_count": 52
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Get Previous Day Data"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def get_sensors_data_day(read_date_dt=datetime.now()-timedelta(days=1), groups_cols_ids=groups_cols_ids, urls_data=urls_data):\r\n",
					"    \r\n",
					"    all_dfs_sensors_day = []\r\n",
					"    \r\n",
					"    for cols_ids, url_data in zip(groups_cols_ids, urls_data):\r\n",
					"        df_sensor_data_day = pd.DataFrame(columns=cols_ids)\r\n",
					"        \r\n",
					"        time_ranges = [read_date_dt.replace(hour=i, minute=0, second=0, microsecond=0) for i in [0, 6, 12, 18]]\r\n",
					"        for tr in time_ranges:\r\n",
					"            dt_from = tr - timedelta(hours=6)\r\n",
					"            dt_to = tr\r\n",
					"            #print(url_data + f\"/?limit=5&from={dt_from.strftime('%d/%m/%YT%H:%M:%S')}&to={(dt_to).strftime('%d/%m/%YT%H:%M:%S')}\")\r\n",
					"            data_day = extract_data_from_api(url_data + f\"/?limit=5&from={dt_from.strftime('%d/%m/%YT%H:%M:%S')}&to={(dt_to + timedelta(hours=6)).strftime('%d/%m/%YT%H:%M:%S')}\")\r\n",
					"            \r\n",
					"            for sensor in data_day[\"sensors\"]:\r\n",
					"                sensor_id = sensor[\"sensor\"]\r\n",
					"                #print(\"sensor_id:\", sensor_id, end=\" \")\r\n",
					"                # averaging up to the last 5 reads to get a more steady value\r\n",
					"                if sensor[\"observations\"]:\r\n",
					"                    value = np.mean([float(obs[\"value\"]) for obs in sensor[\"observations\"]]).round(4)\r\n",
					"                    df_sensor_data_day.loc[dt_to, sensor_id] = value\r\n",
					"                \r\n",
					"        all_dfs_sensors_day.append(df_sensor_data_day)\r\n",
					"        \r\n",
					"    return all_dfs_sensors_day\r\n",
					"print(get_sensors_data_day())\r\n",
					""
				],
				"execution_count": 53
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": true
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"\r\n",
					"# def get_sensors_data_day(read_date_dt=datetime.now() - timedelta(days=1), groups_cols_ids=None, urls_data=None):\r\n",
					"#     if urls_data is None:\r\n",
					"#         urls_data = [\r\n",
					"#             'http://aca-web.gencat.cat/sdim2/apirest/data/EMBASSAMENT-EST',\r\n",
					"#             'http://aca-web.gencat.cat/sdim2/apirest/data/AFORAMENT-EST',\r\n",
					"#             'http://aca-web.gencat.cat/sdim2/apirest/data/PLUVIOMETREACA-EST',\r\n",
					"#             'http://aca-web.gencat.cat/sdim2/apirest/data/PIEZOMETRE-EST'\r\n",
					"#         ]\r\n",
					"    \r\n",
					"#     if groups_cols_ids is None:\r\n",
					"#         groups_cols_ids = [\r\n",
					"#             reservoir_sensors_ids,\r\n",
					"#             gauge_sensors_ids,\r\n",
					"#             pluviometer_sensors_ids,\r\n",
					"#             piezometer_sensors_ids\r\n",
					"#         ]\r\n",
					"#     all_dfs_sensors_day = []\r\n",
					"    \r\n",
					"#     time_ranges = [read_date_dt.replace(hour=i, minute=0, second=0, microsecond=0) for i in [0, 6, 12, 18]]\r\n",
					"    \r\n",
					"#     for cols_ids, url_data in zip(groups_cols_ids, urls_data):\r\n",
					"#         # List to collect temporary DataFrames\r\n",
					"#         temp_dfs = []\r\n",
					"        \r\n",
					"#         for tr in time_ranges:\r\n",
					"#             dt_from = tr - timedelta(hours=6)\r\n",
					"#             dt_to = tr\r\n",
					"#             data_day = extract_data_from_api(url_data + f\"/?limit=5&from={dt_from.strftime('%d/%m/%YT%H:%M:%S')}&to={(dt_to + timedelta(hours=6)).strftime('%d/%m/%YT%H:%M:%S')}\")\r\n",
					"            \r\n",
					"#             # Create a temporary DataFrame to store current time range data\r\n",
					"#             temp_data = []\r\n",
					"#             for sensor in data_day[\"sensors\"]:\r\n",
					"#                 sensor_id = sensor[\"sensor\"]\r\n",
					"#                 if sensor[\"observations\"]:\r\n",
					"#                     value = np.mean([float(obs[\"value\"]) for obs in sensor[\"observations\"]]).round(4)\r\n",
					"#                     temp_data.append(pd.DataFrame({sensor_id: value}, index=[dt_to]))\r\n",
					"            \r\n",
					"#             # Concatenate current time range data\r\n",
					"#             if temp_data:\r\n",
					"#                 temp_dfs.append(pd.concat(temp_data, axis=1))\r\n",
					"        \r\n",
					"#         # Concatenate all temporary DataFrames to create the final DataFrame for this URL\r\n",
					"#         if temp_dfs:\r\n",
					"#             df_sensor_data_day = pd.concat(temp_dfs, axis=0).reindex(columns=cols_ids)\r\n",
					"#             all_dfs_sensors_day.append(df_sensor_data_day)\r\n",
					"    \r\n",
					"#     return all_dfs_sensors_day\r\n",
					"\r\n",
					"# def display_data_frames(data_frames):\r\n",
					"#     for idx, df in enumerate(data_frames):\r\n",
					"#         print(f\"DataFrame {idx + 1}:\")\r\n",
					"#         display(df)\r\n",
					"#         print(\"-\" * 80)  # Print a separator line for better readability\r\n",
					"\r\n",
					"# # Assume `get_sensors_data_day()` has been called and its output is stored in a variable:\r\n",
					"# dfs = get_sensors_data_day()\r\n",
					"\r\n",
					"# # Display the DataFrames\r\n",
					"# display_data_frames(dfs)"
				],
				"execution_count": 54
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Sensor Reads Data Path"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"\r\n",
					"# Importing all previous data\r\n",
					"# Define the list of file paths for old data\r\n",
					"#DATA_PATH = f\"abfss://waterstorageaccount@projectwatercrisis.dfs.core.windows.net/BronzeLayer/{FolderName}\"\r\n",
					"DATA_PATH = f\"abfss://waterstorageaccount@projectwatercrisis.dfs.core.windows.net/BronzeLayer/{FolderName}\"\r\n",
					"list_paths_old_data = [\r\n",
					"    DATA_PATH + f\"/{category[0]}{sensors_end}.csv\",\r\n",
					"    DATA_PATH + f\"/{category[1]}{sensors_end}.csv\",\r\n",
					"    DATA_PATH + f\"/{category[2]}{sensors_end}.csv\",\r\n",
					"    DATA_PATH + f\"/{category[3]}{sensors_end}.csv\",\r\n",
					"]\r\n",
					"\r\n",
					"# Check if the first file exists using mssparkutils\r\n",
					"if mssparkutils.fs.exists(list_paths_old_data[0]):\r\n",
					"    print(\"Previous Files Found, Loading now \")\r\n",
					"    #list_all_old_data_s = [spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(path) for path in list_paths_old_data]\r\n",
					"    list_all_old_data = [pd.read_csv(path_old_data, parse_dates=[0], index_col=0) for path_old_data in list_paths_old_data]\r\n",
					"else:\r\n",
					"    print(\"No existing File, Initiate first Run\")\r\n",
					"    #list_all_old_data_s = None\r\n",
					"    list_all_old_data = None\r\n",
					"\r\n",
					""
				],
				"execution_count": 55
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### New Df or Append Data\r\n",
					"For the first time, the data is needs to populate the df thus takes a long time."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"\r\n",
					"# If it's the first run and there is no data from previous days:\r\n",
					"# Example check if both variables are None\r\n",
					"#if list_all_old_data_s is None and list_all_old_data_p is None:\r\n",
					"if list_all_old_data is None:\r\n",
					"    print(\"variables are None\")\r\n",
					"    # Checking if the directory doesn't exists and create folder\r\n",
					"    if not mssparkutils.fs.exists(meta_path):\r\n",
					"        mssparkutils.fs.mkdirs(meta_path)\r\n",
					"    # Saving metadata in Dataset for the first time\r\n",
					"\r\n",
					"    reservoir_sensors_metadata.to_csv(f\"{meta_path}/reservoir_sensors_metadata.csv\", index=False)\r\n",
					"    gauge_sensors_metadata.to_csv(f\"{meta_path}/gauge_sensors_metadata.csv\", index=False)\r\n",
					"    pluviometer_sensors_metadata.to_csv(f\"{meta_path}/pluviometer_sensors_metadata.csv\", index=False)\r\n",
					"    piezometer_sensors_metadata.to_csv(f\"{meta_path}/piezometer_sensors_metadata.csv\", index=False)\r\n",
					"\r\n",
					"    list_all_data = [[] for _ in range(4)]\r\n",
					"    \r\n",
					"    # Requesting the data for all sensors for the last 3 months, which is the max. stored in the public ACA API\r\n",
					"    for d in tqdm(range(89, 0, -1)):\r\n",
					"        print(d)\r\n",
					"        read_date_dt = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(days=d)\r\n",
					"        all_dfs_sensors_day = get_sensors_data_day(read_date_dt=read_date_dt)\r\n",
					"        \r\n",
					"        for i in range(4):\r\n",
					"            list_all_data[i].append(all_dfs_sensors_day[i])\r\n",
					"        \r\n",
					"    list_all_old_data = [pd.concat(dfs_sensor_data, axis=0) for dfs_sensor_data in list_all_data]\r\n",
					"    print(\"First Load Compleete\")\r\n",
					"    \r\n",
					"else:  \r\n",
					"    # Normal daily update\r\n",
					"    read_date_dt = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(days=1)\r\n",
					"    print(\"Old data found, requesting and updating previous day's sensor reads...\")\r\n",
					"\r\n",
					"    last_dfs_sensors_day = get_sensors_data_day(read_date_dt=read_date_dt)\r\n",
					"\r\n",
					"    list_all_old_data = [pd.concat([old_df_sensor, last_df_sensor], axis=0) for old_df_sensor, last_df_sensor in zip(list_all_old_data, last_dfs_sensors_day)]\r\n",
					"    list_all_old_data = [df_sensor.sort_index(ascending=False) for df_sensor in list_all_old_data]\r\n",
					"    list_all_old_data = [df_sensor[~df_sensor.index.duplicated(keep=\"first\")] for df_sensor in list_all_old_data]\r\n",
					"    print(\"Daily Update Complete\")\r\n",
					""
				],
				"execution_count": 56
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Data Loading"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# print(f\"{sensors_reads_path}/{category[0]}{sensors_end}\")"
				],
				"execution_count": 57
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"\r\n",
					"if list_all_old_data is not None:\r\n",
					"    for category, df_sensor_data in zip(category, list_all_old_data):\r\n",
					"        print(f\"DataFrame for {category}:\")\r\n",
					"        print(df_sensor_data)\r\n",
					"        df_sensor_data.to_csv(f\"{sensors_reads_path}/{category}{sensors_end}.csv\")\r\n",
					"        print(f\"CSV file for {category} successfully stored\")\r\n",
					"else:\r\n",
					"    print(\"list_all_old_data is None. Please initialize it before iterating.\")\r\n",
					""
				],
				"execution_count": 58
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": 75
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## ML Ops"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Check if the file path exists using mssparkutils\r\n",
					"if not mssparkutils.fs.exists(f\"{ml_path}\"):\r\n",
					"    print(f\"ML Folder Not Found, Creating One Now \")\r\n",
					"    mssparkutils.fs.mkdirs(f\"{ml_path}\")\r\n",
					"    print(f\"ML Folder Created\")\r\n",
					"else:\r\n",
					"    print(f\"ML Folder Found, Proceeding with code\")"
				],
				"execution_count": 59
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Meta Files"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Paths to the CSV files\r\n",
					"meta_files = [\r\n",
					"    f\"{meta_path}/{category[0]}{meta_end}.csv\",\r\n",
					"    f\"{meta_path}/{category[1]}{meta_end}.csv\",\r\n",
					"    f\"{meta_path}/{category[2]}{meta_end}.csv\",\r\n",
					"    f\"{meta_path}/{category[3]}{meta_end}.csv\"\r\n",
					"]\r\n",
					"\r\n",
					"# Add column, read and convert each CSV to a Pandas DataFrame\r\n",
					"meta_dfs = []\r\n",
					"for f, path in enumerate(meta_files):\r\n",
					"    df = pd.read_csv(path)\r\n",
					"    df['sensor_type'] = category[f]\r\n",
					"    meta_dfs.append(df)\r\n",
					"\r\n",
					"# Combine DataFrames\r\n",
					"combined_meta = pd.concat(meta_dfs, ignore_index=True)\r\n",
					"\r\n",
					"# Save the DataFrame directly to a CSV file\r\n",
					"combined_meta.to_csv(f\"{ml_path}/combined_meta.csv\", index=False)\r\n",
					"\r\n",
					"print(combined_meta.head())\r\n",
					""
				],
				"execution_count": 60
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Sensor Files"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"sensor_files = [\r\n",
					"    f\"{sensors_reads_path}/{category[0]}{sensors_end}.csv\",\r\n",
					"    f\"{sensors_reads_path}/{category[1]}{sensors_end}.csv\",\r\n",
					"    f\"{sensors_reads_path}/{category[2]}{sensors_end}.csv\",\r\n",
					"    f\"{sensors_reads_path}/{category[3]}{sensors_end}.csv\"\r\n",
					"]\r\n",
					"\r\n",
					"\r\n",
					"# Read and reshape each CSV file\r\n",
					"sensor_dfs = []\r\n",
					"for path in sensor_files:\r\n",
					"    df = pd.read_csv(path)\r\n",
					"    \r\n",
					"    # Assuming the first column is the date column, rename it if necessary\r\n",
					"    first_column = df.columns[0]\r\n",
					"    df.rename(columns={first_column: 'Date'}, inplace=True)\r\n",
					"    \r\n",
					"    # Melt the DataFrame, keeping the 'Date' column fixed\r\n",
					"    melted_df = pd.melt(df, id_vars=['Date'], var_name='Sensor_Type', value_name='Reading')\r\n",
					"    sensor_dfs.append(melted_df)\r\n",
					"\r\n",
					"# Combine all melted DataFrames\r\n",
					"combined_sensor = pd.concat(sensor_dfs, ignore_index=True)\r\n",
					"sensor_dfs = [pd.read_csv(path) for path in sensor_files]\r\n",
					"\r\n",
					"combined_sensor.to_csv(f\"{ml_path}/combined_sensor.csv\", index=False)\r\n",
					"\r\n",
					"# Print a sample to verify\r\n",
					"print(combined_sensor.head())"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Join"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Perform a full outer join on id\r\n",
					"raw_data = pd.merge(combined_meta, combined_sensor, left_on='sensor_id', right_on='Sensor_Type', how='outer')\r\n",
					"\r\n",
					"# Remove the 'Sensor_Type' column as it is redundant and save\r\n",
					"raw_data.drop('Sensor_Type', axis=1, inplace=True)\r\n",
					"raw_data.to_csv(f\"{ml_path}/raw_data.csv\", index=False)\r\n",
					"\r\n",
					"print(raw_data.head())"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Statistics"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Iterate through each file and its corresponding category\r\n",
					"for file, cat in zip(sensor_files, category):\r\n",
					"    df = pd.read_csv(file)\r\n",
					"    print(f\"Sensor: {cat}\")  # Printing the category instead of the file path\r\n",
					"    print(\"Number of rows:\", df.shape[0])\r\n",
					"    print(\"Number of columns:\", df.shape[1])\r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Iterate through each file and its corresponding category\r\n",
					"for file, cat in zip(meta_files, category):\r\n",
					"    df = pd.read_csv(file)\r\n",
					"    print(f\"Meta: {cat}\")  # Printing the category instead of the file path\r\n",
					"    print(\"Number of rows:\", df.shape[0])\r\n",
					"    print(\"Number of columns:\", df.shape[1])\r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Count the number of rows and columns\r\n",
					"sm_rows = combined_sensor.shape[0]\r\n",
					"sm_columns = combined_sensor.shape[1]\r\n",
					"\r\n",
					"print(\"Number of Sensor rows:\", sm_rows)\r\n",
					"print(\"Number of Sensor columns:\", sm_columns)\r\n",
					"print()\r\n",
					"\r\n",
					"\r\n",
					"# Count the number of rows and columns\r\n",
					"cm_rows = combined_meta.shape[0]\r\n",
					"cm_columns = combined_meta.shape[1]\r\n",
					"\r\n",
					"print(\"Number of Meta rows:\", cm_rows)\r\n",
					"print(\"Number of Meta columns:\", cm_columns)\r\n",
					"print()\r\n",
					"\r\n",
					"\r\n",
					"# Count the number of rows and columns\r\n",
					"snum_rows = raw_data.shape[0]\r\n",
					"snum_columns = raw_data.shape[1]\r\n",
					"\r\n",
					"print(\"Number of RawData rows:\", snum_rows)\r\n",
					"print(\"Number of RawData columns:\", snum_columns)\r\n",
					"print()"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Add Temperature"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def fetch_weather_data(latitude, longitude, start_date, end_date):\r\n",
					"    url = \"https://archive-api.open-meteo.com/v1/archive\"\r\n",
					"    params = {\r\n",
					"        \"latitude\": latitude,\r\n",
					"        \"longitude\": longitude,\r\n",
					"        \"start_date\": start_date,\r\n",
					"        \"end_date\": end_date,\r\n",
					"        \"hourly\": \"temperature_2m\",\r\n",
					"        \"timezone\": \"auto\"\r\n",
					"    }\r\n",
					"    response = requests.get(url, params=params).json()\r\n",
					"    if 'hourly' in response:\r\n",
					"        times = response['hourly']['time']\r\n",
					"        temperatures = response['hourly']['temperature_2m']\r\n",
					"        return pd.to_datetime(times, utc=True), temperatures\r\n",
					"    else:\r\n",
					"        print(\"Failed to retrieve data:\", response)\r\n",
					"        return None, None\r\n",
					"print(\"fetch_weather_data Complete\")\r\n",
					"\r\n",
					"raw_data = pd.read_csv(f\"{ml_path}/raw_data.csv\", low_memory=False)\r\n",
					"raw_data['Date'] = pd.to_datetime(raw_data['Date'])\r\n",
					"# Extract Latitude and Longitude from 'location' every time the script runs\r\n",
					"raw_data[['Latitude', 'Longitude']] = raw_data['location'].str.split(expand=True).astype(float)\r\n",
					"raw_data = raw_data.dropna(subset=['Latitude', 'Longitude'])\r\n",
					"\r\n",
					"\r\n",
					"if not mssparkutils.fs.exists(f\"{ml_path}/raw_data_final.csv\"):\r\n",
					"    print(\"First Run: Fetching and processing Data....\")\r\n",
					"\r\n",
					"    min_date = raw_data['Date'].min().strftime('%Y-%m-%d')\r\n",
					"    max_date = raw_data['Date'].max().strftime('%Y-%m-%d')\r\n",
					"\r\n",
					"    unique_locations = raw_data[['Latitude', 'Longitude']].drop_duplicates()\r\n",
					"    weather_data_list = []\r\n",
					"\r\n",
					"    for index, row in unique_locations.iterrows():\r\n",
					"        latitude = row['Latitude']\r\n",
					"        longitude = row['Longitude']\r\n",
					"        times, temperatures = fetch_weather_data(latitude, longitude, min_date, max_date)\r\n",
					"        if times is not None:\r\n",
					"            location_weather = pd.DataFrame({'Date': times, 'Temperature': temperatures, 'Latitude': latitude, 'Longitude': longitude})\r\n",
					"            weather_data_list.append(location_weather)\r\n",
					"\r\n",
					"    if weather_data_list:\r\n",
					"        complete_weather_data = pd.concat(weather_data_list)\r\n",
					"        complete_weather_data['Date'] = complete_weather_data['Date'].dt.tz_localize(None).dt.strftime('%Y-%m-%d %H:%M:%S')\r\n",
					"        raw_data['Date'] = raw_data['Date'].dt.tz_localize(None).dt.strftime('%Y-%m-%d %H:%M:%S')\r\n",
					"        #complete_weather_data['Date'] = pd.to_datetime(complete_weather_data['Date'], utc=True)\r\n",
					"        #raw_data['Date'] = pd.to_datetime(raw_data['Date'], utc=True)\r\n",
					"        raw_data_final = pd.merge(raw_data, complete_weather_data, how='left', on=['Date', 'Latitude', 'Longitude'])\r\n",
					"        raw_data_final.drop(columns=['Latitude', 'Longitude'], inplace=True)\r\n",
					"\r\n",
					"    raw_data_final.to_csv(f\"{ml_path}/raw_data_final.csv\", index=False)\r\n",
					"    print(raw_data_final.head())\r\n",
					"    \r\n",
					"else:\r\n",
					"    print(\"Not the first run: Fetching data for the previous day...\")\r\n",
					"    previous_day = (datetime.utcnow() - timedelta(days=1)).strftime('%Y-%m-%d')  # Determine 'yesterday's' date\r\n",
					"    new_data_list = []  # Initialize an empty list to store new data frames\r\n",
					"\r\n",
					"    # Iterate over the rows of the dataset to find entries from the previous day\r\n",
					"    for index, row in raw_data.iterrows():\r\n",
					"        if pd.to_datetime(row['Date']).strftime('%Y-%m-%d') == previous_day:\r\n",
					"            latitude = row['Latitude']\r\n",
					"            longitude = row['Longitude']\r\n",
					"            times, temperatures = fetch_weather_data(latitude, longitude, previous_day, previous_day)\r\n",
					"            if times is not None:\r\n",
					"                # Create a DataFrame for each new set of data points fetched\r\n",
					"                new_entries = pd.DataFrame({\r\n",
					"                    'Date': times,\r\n",
					"                    'Temperature': temperatures,\r\n",
					"                    'Latitude': [latitude] * len(times),\r\n",
					"                    'Longitude': [longitude] * len(times)\r\n",
					"                })\r\n",
					"                new_data_list.append(new_entries)  # Append the new DataFrame to the list\r\n",
					"\r\n",
					"    # Use pandas.concat to combine all new data frames if any new data has been fetched\r\n",
					"    if new_data_list:\r\n",
					"        new_data_df = pd.concat(new_data_list, ignore_index=True)  # Concatenate all new data frames into one\r\n",
					"        new_data_df['Date'] = new_data_df['Date'].dt.strftime('%Y-%m-%d %H:%M:%S')\r\n",
					"        raw_data_final = pd.concat([raw_data, new_data_df], ignore_index=True)  # Concatenate the new data with the existing data\r\n",
					"        raw_data_final.drop(columns=['Latitude', 'Longitude'], inplace=True)\r\n",
					"        raw_data_final.to_csv(f\"{ml_path}/raw_data_final.csv\", index=False)  # Save the updated DataFrame back to CSV\r\n",
					"        print(\"Updated data for the previous day.\")\r\n",
					"        print(raw_data_final.head())\r\n",
					"    else:\r\n",
					"        print(\"No new data to update because data is already updated.\")"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Statistics"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"if list_all_old_data is not None:\r\n",
					"    #print(\"Total Rows in First DF:\", len(list_all_old_data[0]))\r\n",
					"    #print(\"Total Sensors:\", sum([len(s.columns) for s in list_all_old_data]))\r\n",
					"    print(\" Sensor Reads\")\r\n",
					"    print(\"From:\", list_all_old_data[0].index[0]) \r\n",
					"    print(\"To:  \", list_all_old_data[0].index[-1])\r\n",
					"else:\r\n",
					"    print(\"list_all_old_data is None. Please initialize it before iterating.\")\r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"if raw_data_final is not None:\r\n",
					"    print(\" ML Data\")\r\n",
					"    print(\"From:\", raw_data_final[0].index[0]) \r\n",
					"    print(\"To:  \", raw_data_final[0].index[-1])\r\n",
					"else:\r\n",
					"    print(\"raw_data_final is None. Please initialize it before iterating.\")\r\n",
					""
				]
			}
		]
	}
}