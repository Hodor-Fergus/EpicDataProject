{
	"name": "test_incremental",
	"properties": {
		"folder": {
			"name": "Other Files"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "WaterSparkPool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "fdbac4ed-e732-4c59-a63f-a1ed3ff7c7bd"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/e6a89ab3-ce27-4148-bad8-97a6fee2010c/resourceGroups/FergusAssam/providers/Microsoft.Synapse/workspaces/water-crisis/bigDataPools/WaterSparkPool",
				"name": "WaterSparkPool",
				"type": "Spark",
				"endpoint": "https://water-crisis.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/WaterSparkPool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.3",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# <div style=\"color:#fff;display:fill;border-radius:15px;background-color:#328ada;text-align:left;letter-spacing:0.1px;overflow:hidden;padding:5px;color:white;overflow:hidden;margin:0;font-size:80%\">Epic Internship</div>"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## <div style=\"color:#fff;display:fill;border-radius:10px;background-color:#328ada;text-align:left;letter-spacing:0.1px;overflow:hidden;padding:20px;color:white;overflow:hidden;margin:0;font-size:80%\"> üîßüìóET Code for the \"üèûÔ∏èCatalonia Water Resource Dailyüö∞\" Dataset.</div>\r\n",
					"\r\n",
					"This Notebook is, **scheduled** to run automatically on a **daily** basis in a pipeline, in order to extract information from the public API of the [ACA - Ag√®ncia Catalana de l'Aigua](https://aca.gencat.cat/) (Catalan Water Agency). This API contains the **real-time value reads from 4 types of sensors** (Reservoir Level Sensors, Gauging Sensors, Pluviometer Sensors, and Piezometer Sensors) placed around the **Catalan region** in Spain. It also contains Metadata information which is extracted and saved in another supplementary Notebook(copy_metadata_monthly), and later imported here.\r\n",
					"\r\n",
					"#### Internship Business Case:\r\n",
					"\r\n",
					"Catalonia is currently experiencing a dry period, characterized by reduced precipitation and lower-than-average water levels in reservoirs and rivers as well as in other regions. Struggling with water reserve levels already, the goal of this internship is to come up with actionable insights that will help the government make informed decisions and solutions that will benefit the community and the region at large.\r\n",
					"\r\n",
					"##### Note:\r\n",
					"Data is uploaded from the sensors at different time intervals but the data is collected and average taken for the closet time on all the 4 categories.\r\n",
					"\r\n",
					"## <div style=\"color:#fff;display:fill;border-radius:15px;background-color:#328ada;text-align:left;letter-spacing:0.1px;overflow:hidden;padding:5px;color:white;overflow:hidden;margin:0;font-size:80%\"></div>\r\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Parameters and Libraries"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Create a parameter that will be used to change read folder and mode dynamically in the pipeline"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"# Get the version of the folder from the Bronze layer and this will be used as a parameter in the pipeline (Toggled as a parameter)\r\n",
					"#oldFolder = \"Version_3165\"\r\n",
					"FolderName = \"Version_15042024\"\r\n",
					"Mode = \"overwrite\" \r\n",
					"MetaFolder = \"Metadata\"\r\n",
					"category = ['reservoir','gauge','pluviometer', 'piezometer']\r\n",
					"meta_end = '_sensors_metadata'\r\n",
					"meta_path = f\"abfss://waterstorageaccount@projectwatercrisis.dfs.core.windows.net/BronzeLayer/{FolderName}/{MetaFolder}\"\r\n",
					"sensors_reads_path = f\"abfss://waterstorageaccount@projectwatercrisis.dfs.core.windows.net/BronzeLayer/{FolderName}\"\r\n",
					"sensors_end = '_sensors_reads'\r\n",
					"#For the first time it overwrites and for subsequent notebooks, it apppends which is adjusted via the pipeline\r\n",
					"# Also live data can be extracted from the API."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Check If Folder Exists"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Check if the file path exists using mssparkutils\r\n",
					"if not mssparkutils.fs.exists(f\"{sensors_reads_path}\"):\r\n",
					"    print(f\"Folder {FolderName} Not Found, Creating One Now \")\r\n",
					"    mssparkutils.fs.mkdirs(f\"{sensors_reads_path}\")\r\n",
					"else:\r\n",
					"    print(f\"Folder {FolderName} Found, Proceeding with code\")"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Generate the dataset Folder incrementally"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"FolderName_SeperatorRemoved = FolderName.replace(\"_\",\"\")\r\n",
					"VersionName = FolderName.split('_')[0]\r\n",
					"VersionNum = FolderName.split('_')[1]"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Import Libraries"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql import SparkSession\r\n",
					"from pyspark.sql import functions as F, Row\r\n",
					"from pyspark.sql.functions import lit,col,regexp_replace,to_date,count,sum,coalesce,to_timestamp, current_date, date_sub #add constant or literal value as a new column to the DataFrame. #regexp_replace to replace a character in a string\r\n",
					"from pyspark.sql.types import DateType\r\n",
					"from datetime import datetime, timedelta\r\n",
					"from tqdm.notebook import tqdm\r\n",
					"import re\r\n",
					"#from pathlib import Path\r\n",
					"import matplotlib.pyplot as plt\r\n",
					"from scipy.stats import zscore\r\n",
					"import pandas as pd\r\n",
					"import numpy as np\r\n",
					"import requests,json,os\r\n",
					"from notebookutils import mssparkutils\r\n",
					"spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\r\n",
					"#If you check the schema table, you will notice the column Timestamp is stored as (M.D.Y.hh.mm). \r\n",
					"#inorder to handle unforseen exception error. eg October, Oct ,oct, 10, we use the timeParserPolicy=LEGACY to fix this."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Create Connections"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Spark Session & blob connection\r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Create Spark Session and connection to blob storage. It is created in the Notebook and not in the pipeline. \r\n",
					"#You can only set Spark configuration properties that start with the spark.sql prefix.\r\n",
					"# \"\"\"You can have many spark sessions but only 1 spark context (like spark driver). It is better in the notebook because\r\n",
					"# It already has access to the underlying Spark features for creating RDD and dataframe.\"\"\"\r\n",
					"\r\n",
					"blob_account = \"waterstorageaccount\"\r\n",
					"blob_container = \"BronzeLayer\"\r\n",
					"\r\n",
					"sc = SparkSession.builder.getOrCreate()\r\n",
					"#getConnectionString()\r\n",
					"token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\r\n",
					"dls_sas_token = token_library.getConnectionString(\"water-crisis-WorkspaceDefaultStorage\")\r\n",
					"spark.conf.set(\r\n",
					"    'fs.azure.sas.{}.{}.blob.core.windows.net' .format(blob_container, blob_account),\r\n",
					"    dls_sas_token)\r\n",
					"\r\n",
					"#https://projectwatercrisis.dfs.core.windows.net/waterstorageaccount/BronzeLayer/Version_322/gauge_sensors_reads.csv\r\n",
					""
				]
			}
		]
	}
}